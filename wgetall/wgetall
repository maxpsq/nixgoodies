#!/bin/bash

#
# wgetall v0.6.0
# LAST EDIT: 2017-08-24 00:15 CEST
#
# AUTHOR: Massimo Pasquini
#
# SYNOPSYS:
# This script looks for links to some given 
# file types or file extensions in a web page 
# referred by a given URL and then 
# downloads all the matching links to the current
# directory.
#
# DEPENDENCIES:
# for this script to run succesfully, you need 
# to have this tools installed on your machine:
#
# * lynx  [http://lynx.browser.org/]
# * wget  [http://www.gnu.org/software/wget/wget.html]
# * python

unprotected_audio='m4a|ogg|flac'
protected_audio='m4p'
lossy_audio='mp3|m4a|ogg|m4p'
lossless_audio='flac'
audio="$lossy_audio|$lossless_audio"
video='mkv|avi'

function usage()
{
  usage="Usage:

`basename $0`  -h|--help  -t|--type type  -e|--extension ext  -r|--regex regex  -l|--list listfile  -y|--yes-to-all  -d|--dump  url
  -h|--help            : show this help
  -t|--type type       : download all file belonging to a specific group (see below)
  -e|--extension ext   : download all the files having the given extension
  -r|--regex regex     : download all the files matching the given regular expression pattern
  -l|--list listfile   : download all the URLs listed in the given file
  -y|--yes-to-all      : download all the matching URLs without asking for confirmation
  -d|--dump            : extract the matching links 
  url                  : the URL you want to scrape for collecting links

groups:
  unprotected_audio:  $unprotected_audio
  protected_audio:    $protected_audio
  lossy_audio:        $lossy_audio
  lossless_audio:     $lossless_audio
  audio:              $audio
  video:              $video
"
  echo "$usage"
  exit 1
}

cleanUp() 
{
  rm -f "$downlist"
}

help=0
regex=''
optCnt=0
force=0
dump=0
removedownlist=1
while [[ $# > 0 ]]
do
  key="$1"

  case $key in
    -h|--help)
      help=1
      ;;
    -e|--extension)
      ext="$2"; optCnt=$((optCnt+1))
      shift # past argument
      ;;
    -t|--type)
      type="$2"; optCnt=$((optCnt+1))
      shift # past argument
      ;;
    -r|--regex)
      regex="$2"; optCnt=$((optCnt+1))
      shift # past argument
      ;;
    -l|--list)
      downlist="$2"; optCnt=$((optCnt+1))
      removedownlist=0 ; url='filelist'  # just to avoid params check to fail
      shift # past argument
      ;;
    -y|--yes-to-all)
      force=1
      ;;
    -d|--dump)
      dump=1 
      ;;
    *)
      url=$key
      ;;
  esac
  shift # past argument or value
done

if [ "$help" -eq 1 ]; then
  usage
fi

if [ -z "$url" ]; then
  echo "No URL specified" ; usage
fi
if [ 1 -ne "$optCnt" ] ; then
  echo "Specify one among 'type', 'extension', 'regex' or 'list'" ; usage
fi
if [ ! -z "$downlist" ]; then
  dump=0
fi
if [ -z "$url" ]; then
  force=1 ; removedownlist=0 ; downlist=''
fi

if [ ! -z "$type" ]; then
  case "$type" in
    audio) ext=$audio
    ;;
    lossy_audio) ext=$lossy_audio
    ;;
    lossless_audio) ext=$lossless_audio
    ;;
    video) ext=$video
    ;;
    *) echo "Unmatched type $type" ; usage
    ;;
  esac
  regex="\.($ext)"'$'
else
  if [ ! -z "$ext" ]; then
    regex="\.$ext"'$'
  fi
fi

shopt -s expand_aliases

alias urldecode='python -c "import sys, urllib as ul;[sys.stdout.write(ul.unquote_plus(lnk)) for lnk in sys.stdin]"'
alias urlencode='python -c "import sys, urllib as ul;[sys.stdout.write(ul.quote(lnk,safe=\"\n/:\")) for lnk in sys.stdin]"'

#echo "Looking for $ext files..."

if [ -z "$downlist" ]; then
  downlist="downlist_$$.tmp"

  # Extracting matching links
  if [ 0 -eq "$dump" ] ; then
    lynx -listonly -nonumbers -dump "$url" | egrep "$regex" | urldecode | urlencode > "$downlist"
  else
    lynx -listonly -nonumbers -dump "$url" | egrep "$regex" | urldecode | urlencode
  fi
  if [ 0 -eq "$force" ]; then
    more "$downlist"
    echo -n 'Proceede downloading? (y/n): '
    read answer
    if echo "$answer" | grep -iq "^y" ; then
      echo ""
    else
      echo "Exiting... "; 
      cleanUp;
      exit 0
    fi
  fi
fi

if [ 0 -eq "$dump" ] ; then
# Downloading files
# --continue                resume a previously interrupted downloading
# --no-check-certificate    skip checking SSL certificate: useful on sites with broken certificate
#                           NOTICE you also need to set FORCE_SSL_PROMPT=no in lynx.cfg file
  wget --continue --no-check-certificate -i "$downlist"
fi

if [ 1 -eq "$removedownlist" ]; then
  # Removing list of matching files
  cleanUp
fi

exit 0
